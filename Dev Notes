# The main factor for sys_admins is mem_limit that is that actual memory footprint. 
# Disk_size is purely the virtual uncompressed of the comp_alg and becomes a strange question.
# If you hit comp gold then why limit this as mem_limit controls physical mem usage and with log drives also doesn't include the large overhead of super/blocks...
#
# Max steams is always set to CPU core count and maybe in some architecture you would wish to give a priority to big-cores or exclude little-cores.
# It doesn't exist so its prob as pointless as creating multiple devices as all that does is create smaller partitions if the mem_alg dvision by num_devices is used
#
# By default, the Linux kernel swaps in 8 pages of memory at a time. When using ZRAM, the incremental cost of reading 1 page at a time is negligible
# and may help in case the device is under extreme memory pressure.
# echo "0" /proc/sys/vm/page-cluster
# This prob should be optional as effects overall /sys/vm prob call it TWEAK_PAGE_CLUSTER
#
# Much of the rest of the Zram kernal documentation https://www.kernel.org/doc/Documentation/blockdev/zram.txt is generally a confusion but mostly,
# confusion over implementation and benefit.
# The above are maybe not apparent but for me its a single device controlled by mem_limit
#
# The only other consideration is initialistaion and to check no previous service has loaded the zram module by checking '/sys/class/zram-control' exists
# If ! then create /dev/zram0 and then precede to use the hot_plug system at cat /sys/class/zram-control/hot_add
#
# Also any /proc/crypto alg can be used but only lzo/lz4 have zramctl test strings so all others display as blank.
# If non existant or null is supplied for comp_alg then lzo will be used so if zramctl shows a blank comp_alg and you results come in as expected its likely
# comp_alg choice was sucessful. You can pretty much guarantee lzo/lz4/deflate and deflate might not be the latest or greatest its likely to exist and fit
# purpose for log compression.
# LZ4 without doubt is the best alg for zram but from testing it would seem LZO often has better compiler optimistaion and you could see them swap places in choice
# of preference
#
# How to test
# sudo apt-get install lzop liblz4-tool
# https://www.lzop.org/lzop_man.php
# https://manpages.debian.org/stretch/liblz4-tool/lz4.1.en.html
# Grab a decent file size compress & uncompress in a loop and see which has the best optimisation for your distro.
# Or just monitor cat /proc/loadavg with zram running as timed operation shows much, but actual load is better
#
# Synthetic tests
# sudo apt-get install stress stress-ng stress-ng has better metrics but seems harsher on process and would sometimes seem to cause crashes.
#
# stress-ng has --metrics stress does not
# stress --vm 2 --vm-bytes 128M --timeout 60s
# The --vm 2 will start N workers (2 workers) continuously calling mmap/munmap and writing to the allocated memory.
# Note that this can cause systems to trip the kernel OOM killer on Linux systems if not enough physical memory and swap is not available
# Give it a test -vm n for various workers making sure --vm-bytes * -vm n doesn't exceed total memory
#
# Zram is an amazing option but it adds load, the load is minimal but at points of high stress any further load is a problem.
#
# By dropping disk based swap you can employ the advantages of that by upping the vm_swappiness from 60 to 80 which gives a balance
# As overall long term your loadav due to better zram swap mechanism will prob drop 20% with vm_swappiness of 80 - 100
# At extreme load with zram as log2ram sets things might garner a couple of percent such as boot anything less than 70 seems to make no difference.
#
# vm_swappiness in conjubction with zram can have 20% benfits and also at extreme load be just as detrimental.
# the static nature of vm_swappiness means its always a comphremise for all situations on a dynamic system.
# so log2ram will run a schedule check current load and shift vm_swappiness from 70 - 100 dynamically to get the best operation for the current situation.
# Its a simple cron job and should really help minimise comphremise loss and vm_swappiness does not need to be set as log2ram if zram is enabled.
# Will apply optimised vm_swappiness dynamically at boot vm_swappiness = 70 and as load decreases it wil rise.
# The sheduler is constant so dynmic allocation of vm_swappiness = 70 will continue until package is removed
# PS apols for all the /etc/log2ram.conf settings and for most uses the default settings will work great
# if you like to tweak performance than all those are available and not locked in script.
# All you need to do is set:-
# ZLTG flag means use ZRAM ramdisk for log2ram true=enable / false=disable
# ZLTG=false means it will use tmpfs
# ZL2R=true means a compressed zram (deflate) drive for /var/log with minimal flash wear on low frequency write to disk ob boot/ shutdown and when nearing full
# SWAP_DEVICES=0 Disables zram swaps
# SWAP_DEVICES=1 as there is zero point of multiple smaller swap partitions when each device has multiple streams
# Thats it as go with the defaults otherwise tweak to your desire

